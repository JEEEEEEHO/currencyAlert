# Spark Worker용 Dockerfile
# Python 버전을 Airflow 컨테이너와 일치시키기 위해 Python 3.12를 추가 설치합니다
FROM apache/spark:3.5.1-scala2.12-java17-python3-ubuntu

USER root

# Python 3.12 설치
# Spark worker 컨테이너의 기본 Python은 3.10인데,
# Airflow 컨테이너의 Python 3.12와 버전을 맞추기 위해 Python 3.12를 설치합니다.
# Ubuntu 22.04는 기본적으로 Python 3.10을 제공하지만, Python 3.12를 설치할 수 있습니다.
# deadsnakes PPA를 사용하는 대신, 더 안정적인 방법으로 설치합니다.
RUN apt-get update && \
    apt-get install -y --no-install-recommends software-properties-common && \
    add-apt-repository -y ppa:deadsnakes/ppa && \
    apt-get update && \
    # python3.12-distutils는 Python 3.12부터는 별도 패키지가 아닙니다
    # distutils는 Python 표준 라이브러리에 포함되어 있어서 별도 설치가 필요 없습니다
    apt-get install -y --no-install-recommends python3.12 python3.12-venv python3.12-dev && \
    # pip3.12 설치
    curl -sS https://bootstrap.pypa.io/get-pip.py | python3.12 && \
    # Python 3.12를 기본 python3로 설정
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1 && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 2 && \
    rm -rf /var/lib/apt/lists/*

# Python 3.12를 기본으로 설정
# update-alternatives를 사용하여 python3 명령어가 python3.12를 가리키도록 설정합니다
RUN update-alternatives --set python3 /usr/bin/python3.12

# 환경 변수 설정: Spark가 Python 3.12를 사용하도록 설정
# 이 환경 변수들은 Spark가 Python 인터프리터를 찾을 때 사용됩니다
# PYSPARK_PYTHON: Spark executor(워커 노드)에서 사용할 Python 경로
# PYSPARK_DRIVER_PYTHON: Spark driver(작업 제출 노드)에서 사용할 Python 경로
ENV PYSPARK_PYTHON=/usr/bin/python3.12
ENV PYSPARK_DRIVER_PYTHON=/usr/bin/python3.12

USER spark

