services:
  mongo:
    image: mongo:6
    container_name: fx_mongo
    restart: unless-stopped
    ports: ["27017:27017"]
    volumes:
      - mongo_data:/data/db

  redis:
    image: redis:7
    container_name: fx_redis
    restart: unless-stopped
    ports: ["6379:6379"]

  backend:
    build: ./backend
    container_name: fx_backend
    env_file: ./backend/.env
    environment:
      - MONGODB_URI=mongodb://mongo:27017/fx_alert
      - REDIS_URL=redis://redis:6379/0
      - CORS_ALLOW_ORIGINS=http://localhost:5173
    ports: ["8000:8000"]
    depends_on: [mongo, redis]

  celery_worker:
    build: ./backend
    container_name: fx_celery_worker
    command: ["celery", "-A", "app.tasks.currency_tasks.celery_app", "worker", "-l", "INFO"]
    env_file: ./backend/.env
    environment:
      - MONGODB_URI=mongodb://mongo:27017/fx_alert
      - REDIS_URL=redis://redis:6379/0
    depends_on: [backend, mongo, redis]

  celery_beat:
    build: ./backend
    container_name: fx_celery_beat
    command: ["celery", "-A", "app.tasks.currency_tasks.celery_app", "beat", "-l", "INFO"]
    env_file: ./backend/.env
    environment:
      - MONGODB_URI=mongodb://mongo:27017/fx_alert
      - REDIS_URL=redis://redis:6379/0
    depends_on: [backend, mongo, redis]

  frontend:
    build:
      context: ./frontend
      args:
        - VITE_API_BASE_URL=http://backend:8000/api/v1
    container_name: fx_frontend
    ports: ["5173:80"]
    depends_on: [backend]

 # --- Airflow & Spark 서비스  ---
  postgres:
    image: postgres:13
    container_name: airflow_postgres
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks: [fxnet]


  airflow-redis:
    image: redis:7
    container_name: airflow_redis
    ports:
      - "6380:6379"
    networks: [fxnet]

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    container_name: airflow_init
    depends_on:
      - postgres
      - airflow-redis
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://airflow-redis:6379/1
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/${GCP_KEY_FILENAME}
      - ECOS_API_KEY=${ECOS_API_KEY}
    command: >
      bash -c "airflow db init &&
      airflow users create --role Admin --username airflow --password airflow --firstname Anonymous --lastname User --email admin@example.org"
    volumes:
      - ./airflow/keys:/opt/airflow/keys       # ← (선택) 키도 보이게
    networks: [fxnet]   

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    container_name: airflow_webserver
    restart: always
    depends_on:
      - postgres
      - airflow-redis
      - airflow-scheduler
      - spark-master
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/keys:/opt/airflow/keys
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://airflow-redis:6379/1
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/${GCP_KEY_FILENAME}
      - BIGQUERY_PROJECT_ID=${BIGQUERY_PROJECT_ID}
      - GCP_SERVICE_ACCOUNT_KEY_PATH=/opt/airflow/keys/${GCP_KEY_FILENAME}
      - ECOS_API_KEY=${ECOS_API_KEY}
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
    command: ["airflow", "webserver"]
    networks: [fxnet]

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    container_name: airflow_scheduler
    restart: always
    depends_on:
      - postgres
      - airflow-redis
      - spark-master
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/keys:/opt/airflow/keys
      - ./spark_jobs:/opt/spark/work-dir
      - ./spark_jars:/opt/spark-3.5.1-bin-hadoop3/jars-extra  # ← JAR 마운트
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://airflow-redis:6379/1
      - GCP_SERVICE_ACCOUNT_KEY_PATH=/opt/airflow/keys/${GCP_KEY_FILENAME}
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/${GCP_KEY_FILENAME}
      - BIGQUERY_PROJECT_ID=${BIGQUERY_PROJECT_ID}
      - ECOS_API_KEY=${ECOS_API_KEY}
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - PYSPARK_PYTHON=/usr/bin/python3
      - PYSPARK_DRIVER_PYTHON=/usr/bin/python3
    command: ["airflow", "scheduler"]
    networks: [fxnet]

  airflow-worker:
    build:
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env
    container_name: airflow_worker
    restart: always
    depends_on:
      - postgres
      - airflow-redis
      - airflow-scheduler
      - spark-master
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/keys:/opt/airflow/keys
      - ./spark_jobs:/opt/spark/work-dir
      - ./spark_jars:/opt/spark-3.5.1-bin-hadoop3/jars-extra  # ← JAR 마운트
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      - AIRFLOW__CELERY__BROKER_URL=redis://airflow-redis:6379/1
      - GCP_SERVICE_ACCOUNT_KEY_PATH=/opt/airflow/keys/${GCP_KEY_FILENAME}
      - GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/keys/${GCP_KEY_FILENAME}
      - BIGQUERY_PROJECT_ID=${BIGQUERY_PROJECT_ID}
      - ECOS_API_KEY=${ECOS_API_KEY}
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - PYSPARK_PYTHON=/usr/bin/python3
      - PYSPARK_DRIVER_PYTHON=/usr/bin/python3
    command: ["airflow", "celery", "worker"]
    networks: [fxnet]

  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    hostname: spark-master
    ports:
      - "7077:7077"
      - "8081:8080"                  # 외부 8081 -> 내부 8080
    env_file: [.env]
    environment:
      - GCP_SERVICE_ACCOUNT_KEY_PATH=/opt/spark/keys/${GCP_KEY_FILENAME}
      - PYSPARK_PYTHON=/usr/bin/python3.12
      - PYSPARK_DRIVER_PYTHON=/usr/bin/python3.12
    volumes:
      - ./spark_jobs:/opt/spark/work-dir
      - ./airflow/keys:/opt/spark/keys
      - ./airflow/keys:/opt/airflow/keys
      - ./spark_jars:/opt/spark-3.5.1-bin-hadoop3/jars-extra  # JAR 파일들도 마운트
    networks: [fxnet]
    command:
      ["/opt/spark/bin/spark-class","org.apache.spark.deploy.master.Master",
       "--host","spark-master","--port","7077","--webui-port","8080"]

  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker
    depends_on: [spark-master]
    ports:
      - "8082:8080"
    env_file: [.env]
    environment:
      - GCP_SERVICE_ACCOUNT_KEY_PATH=/opt/spark/keys/${GCP_KEY_FILENAME}
      - PYSPARK_PYTHON=/usr/bin/python3.12
      - PYSPARK_DRIVER_PYTHON=/usr/bin/python3.12
    volumes:
      - ./spark_jobs:/opt/spark/work-dir
      - ./airflow/keys:/opt/spark/keys
      - ./airflow/keys:/opt/airflow/keys
      - ./spark_jars:/opt/spark-3.5.1-bin-hadoop3/jars-extra  # JAR 파일들도 마운트
    networks: [fxnet]
    command:
      ["/opt/spark/bin/spark-class","org.apache.spark.deploy.worker.Worker",
       "spark://spark-master:7077"]

volumes:
  mongo_data:
  postgres-db-volume:

networks:
  fxnet: 